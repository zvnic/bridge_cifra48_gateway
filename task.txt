Отлично! Создам обертку для использования DeepSeek API в качестве OpenAI-совместимого эндпоинта. Это позволит использовать его с AI Agent, LangChain, AutoGen и другими инструментами, ожидающими OpenAI API.

1. FastAPI Обертка для OpenAI-совместимости
main.py - Основное приложение
python
import os
import json
import logging
from typing import List, Optional, Dict, Any, Union
from fastapi import FastAPI, HTTPException, Header, UploadFile, File, Form, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import requests
import aiohttp
import asyncio
from datetime import datetime
import uuid
import mimetypes

# Настройка логирования
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="DeepSeek OpenAI-Compatible API",
    description="OpenAI-совместимая обертка для DeepSeek V3 API",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Конфигурация
DEEPSEEK_API_URL = os.getenv("DEEPSEEK_API_URL", "https://bridge-back.admlr.lipetsk.ru/api/v1/completions")
DEFAULT_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
MODEL_NAME = os.getenv("MODEL_NAME", "deepseek-v3")

# Модели Pydantic для OpenAI-совместимости
class ChatMessage(BaseModel):
    role: str  # system, user, assistant
    content: str
    name: Optional[str] = None

class ChatCompletionRequest(BaseModel):
    model: str = Field(default=MODEL_NAME)
    messages: List[ChatMessage]
    temperature: Optional[float] = Field(default=0.7, ge=0, le=2)
    top_p: Optional[float] = Field(default=0.9, ge=0, le=1)
    n: Optional[int] = Field(default=1, ge=1)
    stream: Optional[bool] = False
    stop: Optional[Union[str, List[str]]] = None
    max_tokens: Optional[int] = Field(default=None, ge=1)
    presence_penalty: Optional[float] = Field(default=0, ge=-2, le=2)
    frequency_penalty: Optional[float] = Field(default=0, ge=-2, le=2)
    user: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: Optional[str] = "stop"

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[ChatCompletionChoice]
    usage: Usage

class Model(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "deepseek"

# Вспомогательные функции
async def make_deepseek_request(
    messages: List[Dict[str, Any]],
    api_key: str,
    stream: bool = False,
    temperature: float = 0.7,
    max_tokens: Optional[int] = None,
    file_content: Optional[bytes] = None,
    file_name: Optional[str] = None
) -> Dict[str, Any]:
    """Отправляет запрос к DeepSeek API"""
    
    headers = {
        "X-API-Key": api_key,
        "User-Agent": USER_AGENT,
    }
    
    payload = {
        "messages": messages,
        "model": "deepseek-ai/DeepSeek-V3-0324",
        "stream": stream,
    }
    
    # Добавляем опциональные параметры
    if temperature is not None:
        payload["temperature"] = temperature
    if max_tokens is not None:
        payload["max_tokens"] = max_tokens
    
    try:
        if file_content and file_name:
            # Мультипарт запрос с файлом
            data = aiohttp.FormData()
            data.add_field('messages', json.dumps(messages))
            data.add_field('model', "deepseek-ai/DeepSeek-V3-0324")
            data.add_field('stream', str(stream).lower())
            
            # Определяем MIME тип
            mime_type, _ = mimetypes.guess_type(file_name)
            if not mime_type:
                mime_type = 'application/octet-stream'
            
            data.add_field('file', file_content, filename=file_name, content_type=mime_type)
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    DEEPSEEK_API_URL,
                    headers=headers,
                    data=data
                ) as response:
                    if response.status != 201:
                        error_text = await response.text()
                        logger.error(f"DeepSeek API error: {response.status} - {error_text}")
                        raise HTTPException(
                            status_code=response.status,
                            detail=f"DeepSeek API error: {error_text}"
                        )
                    return await response.json()
        else:
            # Обычный JSON запрос
            headers["Content-Type"] = "application/json"
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    DEEPSEEK_API_URL,
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status != 201:
                        error_text = await response.text()
                        logger.error(f"DeepSeek API error: {response.status} - {error_text}")
                        raise HTTPException(
                            status_code=response.status,
                            detail=f"DeepSeek API error: {error_text}"
                        )
                    return await response.json()
                    
    except Exception as e:
        logger.error(f"Request error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

def transform_to_openai_format(
    deepseek_response: Dict[str, Any],
    request_model: str,
    stream: bool = False
) -> Union[Dict[str, Any], StreamingResponse]:
    """Преобразует ответ DeepSeek в OpenAI-совместимый формат"""
    
    if stream:
        # Для стриминга возвращаем генератор
        async def stream_generator():
            for line in deepseek_response.get("stream_data", "").split("\n"):
                if line.startswith("data: "):
                    data_str = line[6:]
                    if data_str == "[DONE]":
                        yield "data: [DONE]\n\n"
                        break
                    
                    try:
                        chunk = json.loads(data_str)
                        openai_chunk = {
                            "id": chunk.get("id", f"chatcmpl-{uuid.uuid4().hex}"),
                            "object": "chat.completion.chunk",
                            "created": chunk.get("created", int(datetime.now().timestamp())),
                            "model": request_model,
                            "choices": [
                                {
                                    "index": 0,
                                    "delta": chunk.get("choices", [{}])[0].get("delta", {}),
                                    "finish_reason": None
                                }
                            ]
                        }
                        yield f"data: {json.dumps(openai_chunk)}\n\n"
                    except:
                        pass
        
        return StreamingResponse(
            stream_generator(),
            media_type="text/event-stream"
        )
    else:
        # Для обычного ответа
        choices = deepseek_response.get("choices", [])
        if choices:
            message_content = choices[0].get("message", {}).get("content", "")
        else:
            message_content = ""
        
        usage = deepseek_response.get("usage", {})
        
        return {
            "id": deepseek_response.get("id", f"chatcmpl-{uuid.uuid4().hex}"),
            "object": "chat.completion",
            "created": deepseek_response.get("created", int(datetime.now().timestamp())),
            "model": request_model,
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": message_content
                    },
                    "finish_reason": "stop"
                }
            ],
            "usage": {
                "prompt_tokens": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0)
            }
        }

# OpenAI-совместимые эндпоинты

@app.get("/v1/models")
async def list_models():
    """Возвращает список доступных моделей"""
    return {
        "object": "list",
        "data": [
            {
                "id": MODEL_NAME,
                "object": "model",
                "created": int(datetime.now().timestamp()),
                "owned_by": "deepseek"
            }
        ]
    }

@app.post("/v1/chat/completions")
async def create_chat_completion(
    request: ChatCompletionRequest,
    authorization: Optional[str] = Header(None)
):
    """Основной эндпоинт для чат-комплишенов (OpenAI-совместимый)"""
    
    # Извлекаем API ключ
    api_key = DEFAULT_API_KEY
    if authorization and authorization.startswith("Bearer "):
        api_key = authorization[7:]
    
    if not api_key:
        raise HTTPException(status_code=401, detail="API key is required")
    
    # Преобразуем сообщения в формат DeepSeek
    deepseek_messages = []
    for msg in request.messages:
        deepseek_messages.append({
            "role": msg.role,
            "content": msg.content
        })
    
    try:
        # Отправляем запрос к DeepSeek
        response = await make_deepseek_request(
            messages=deepseek_messages,
            api_key=api_key,
            stream=request.stream,
            temperature=request.temperature,
            max_tokens=request.max_tokens
        )
        
        # Преобразуем ответ в OpenAI формат
        if request.stream:
            return transform_to_openai_format(
                {"stream_data": response.text},
                request.model,
                stream=True
            )
        else:
            openai_response = transform_to_openai_format(
                response,
                request.model,
                stream=False
            )
            return JSONResponse(content=openai_response)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/v1/chat/completions/with-file")
async def create_chat_completion_with_file(
    messages: str = Form(...),
    model: str = Form(default=MODEL_NAME),
    temperature: float = Form(default=0.7),
    stream: bool = Form(default=False),
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(None)
):
    """Эндпоинт для работы с файлами"""
    
    api_key = DEFAULT_API_KEY
    if authorization and authorization.startswith("Bearer "):
        api_key = authorization[7:]
    
    if not api_key:
        raise HTTPException(status_code=401, detail="API key is required")
    
    # Читаем файл
    file_content = await file.read()
    
    # Парсим messages
    try:
        messages_list = json.loads(messages)
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid messages JSON")
    
    try:
        response = await make_deepseek_request(
            messages=messages_list,
            api_key=api_key,
            stream=stream,
            temperature=temperature,
            file_content=file_content,
            file_name=file.filename
        )
        
        if stream:
            return transform_to_openai_format(
                {"stream_data": response.text},
                model,
                stream=True
            )
        else:
            openai_response = transform_to_openai_format(
                response,
                model,
                stream=False
            )
            return JSONResponse(content=openai_response)
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

# Health check
@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "deepseek-openai-wrapper"}

# Эндпоинт для тестирования
@app.get("/test")
async def test_endpoint(authorization: Optional[str] = Header(None)):
    """Тестовый эндпоинт для проверки подключения"""
    api_key = DEFAULT_API_KEY
    if authorization and authorization.startswith("Bearer "):
        api_key = authorization[7:]
    
    if not api_key:
        return {"status": "error", "message": "API key is required"}
    
    # Простой тестовый запрос
    test_messages = [{"role": "user", "content": "Привет, ответь 'OK' если ты работаешь."}]
    
    try:
        response = await make_deepseek_request(
            messages=test_messages,
            api_key=api_key,
            stream=False
        )
        return {
            "status": "success",
            "deepseek_response": response,
            "openai_compatible": True
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}
2. Dockerfile для развертывания
dockerfile
FROM python:3.11-slim

WORKDIR /app

# Установка зависимостей
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Копирование исходного кода
COPY . .

# Создание пользователя для безопасности
RUN useradd -m -u 1000 fastapi-user && \
    chown -R fastapi-user:fastapi-user /app
USER fastapi-user

# Порт для приложения
EXPOSE 8000

# Запуск приложения
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]
3. requirements.txt
txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
python-multipart==0.0.6
aiohttp==3.9.1
requests==2.31.0
python-dotenv==1.0.0
4. docker-compose.yml для легкого развертывания
yaml
version: '3.8'

services:
  deepseek-openai-wrapper:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DEEPSEEK_API_URL=https://bridge-back.admlr.lipetsk.ru/api/v1/completions
      - DEEPSEEK_API_KEY=your_api_key_here
      - MODEL_NAME=deepseek-v3
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
5. .env файл для конфигурации
env
# DeepSeek API конфигурация
DEEPSEEK_API_URL=https://bridge-back.admlr.lipetsk.ru/api/v1/completions
DEEPSEEK_API_KEY=your_actual_api_key_here
MODEL_NAME=deepseek-v3

# Настройки сервера
HOST=0.0.0.0
PORT=8000
LOG_LEVEL=INFO
6. Пример использования с AI Agent
Вариант 1: Использование с LangChain
python
# langchain_example.py
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
import os

# Настройка для использования нашей обертки
os.environ["OPENAI_API_KEY"] = "dummy_key"  # Любой ключ, так как мы используем свою обертку
os.environ["OPENAI_API_BASE"] = "http://localhost:8000/v1"

# Создаем модель
llm = ChatOpenAI(
    model="deepseek-v3",
    temperature=0.7,
    max_tokens=1000
)

# Используем как обычную OpenAI модель
messages = [
    SystemMessage(content="Ты полезный ассистент."),
    HumanMessage(content="Привет, расскажи о себе")
]

response = llm.invoke(messages)
print(response.content)
Вариант 2: Использование с OpenAI Python SDK
python
# openai_sdk_example.py
from openai import OpenAI
import os

# Настройка клиента
client = OpenAI(
    api_key="your_deepseek_api_key",  # Ваш реальный ключ DeepSeek
    base_url="http://localhost:8000/v1"
)

# Создание чат-комплишена
response = client.chat.completions.create(
    model="deepseek-v3",
    messages=[
        {"role": "system", "content": "Ты полезный ассистент."},
        {"role": "user", "content": "Привет, как дела?"}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)
Вариант 3: Использование с AutoGen
python
# autogen_example.py
import autogen
from autogen import AssistantAgent, UserProxyAgent
import os

# Конфигурация для использования нашей обертки
config_list = [
    {
        "model": "deepseek-v3",
        "api_key": "your_deepseek_api_key",
        "base_url": "http://localhost:8000/v1",
        "api_type": "open_ai"
    }
]

# Создаем агентов
assistant = AssistantAgent(
    name="deepseek_assistant",
    llm_config={
        "config_list": config_list,
        "temperature": 0.7,
        "timeout": 120,
    }
)

user_proxy = UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=5,
    code_execution_config=False
)

# Запускаем диалог
user_proxy.initiate_chat(
    assistant,
    message="Напиши код на Python для сортировки списка чисел"
)
7. Nginx конфигурация для продакшена
nginx
# /etc/nginx/sites-available/deepseek-wrapper
server {
    listen 80;
    server_name deepseek-api.yourdomain.com;
    
    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Для WebSocket/SSE
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # Таймауты
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
    
    # Безопасность
    client_max_body_size 100M;
}
8. systemd сервис для управления
systemd
# /etc/systemd/system/deepseek-wrapper.service
[Unit]
Description=DeepSeek OpenAI Wrapper
After=network.target

[Service]
User=fastapi-user
Group=fastapi-user
WorkingDirectory=/opt/deepseek-wrapper
Environment="PATH=/opt/deepseek-wrapper/venv/bin"
EnvironmentFile=/opt/deepseek-wrapper/.env
ExecStart=/opt/deepseek-wrapper/venv/bin/uvicorn main:app --host 0.0.0.0 --port 8000
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
9. Скрипт для быстрого запуска
bash
#!/bin/bash
# setup.sh

echo "Установка DeepSeek OpenAI Wrapper..."

# Создание виртуального окружения
python3 -m venv venv
source venv/bin/activate

# Установка зависимостей
pip install -r requirements.txt

# Создание .env файла
if [ ! -f .env ]; then
    cat > .env << EOF
DEEPSEEK_API_URL=https://bridge-back.admlr.lipetsk.ru/api/v1/completions
DEEPSEEK_API_KEY=your_api_key_here
MODEL_NAME=deepseek-v3
HOST=0.0.0.0
PORT=8000
EOF
    echo "Создан файл .env. Пожалуйста, отредактируйте его и укажите ваш API ключ."
fi

echo "Запуск сервера..."
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
10. Тестирование работоспособности
python
# test_integration.py
import requests
import json

# Тест OpenAI-совместимого API
def test_openai_compatibility():
    base_url = "http://localhost:8000/v1"
    api_key = "your_deepseek_api_key"
    
    # 1. Получение списка моделей
    response = requests.get(f"{base_url}/models")
    print("Models:", response.json())
    
    # 2. Создание чат-комплишена
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-v3",
        "messages": [
            {"role": "user", "content": "Привет, кто ты?"}
        ],
        "temperature": 0.7
    }
    
    response = requests.post(
        f"{base_url}/chat/completions",
        headers=headers,
        json=payload
    )
    
    if response.status_code == 200:
        result = response.json()
        print("\nChat Completion Result:")
        print("Response:", result["choices"][0]["message"]["content"])
        print("Tokens used:", result["usage"])
    else:
        print("Error:", response.status_code, response.text)

# Тест стриминга
def test_streaming():
    base_url = "http://localhost:8000/v1"
    api_key = "your_deepseek_api_key"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "deepseek-v3",
        "messages": [
            {"role": "user", "content": "Расскажи короткую историю"}
        ],
        "stream": True,
        "temperature": 0.7
    }
    
    response = requests.post(
        f"{base_url}/chat/completions",
        headers=headers,
        json=payload,
        stream=True
    )
    
    print("\nStreaming response:")
    for line in response.iter_lines():
        if line:
            line_str = line.decode('utf-8')
            if line_str.startswith('data: '):
                data_str = line_str[6:]
                if data_str != '[DONE]':
                    try:
                        chunk = json.loads(data_str)
                        if chunk['choices'][0]['delta'].get('content'):
                            print(chunk['choices'][0]['delta']['content'], end='', flush=True)
                    except:
                        pass

if __name__ == "__main__":
    test_openai_compatibility()
    test_streaming()
Ключевые особенности этой реализации:
Полная совместимость с OpenAI API:

Поддерживаются все основные эндпоинты

Совместимый формат запросов и ответов

Поддержка стриминга (SSE)

Поддержка файлов:

Загрузка документов и изображений

Автоматическое определение MIME-типов

Мультипарт загрузка